# NLP

| **Title**                                                                            | **Year** | **Published** | **Read Date** | **Summary** | **Keywords** |
| ------------------------------------------------------------------------------------ | -------- | ------------- | ------------- | ----------- | ------------ |
| [[Attention is All You Need]]                                                        | 2017     | Arxiv         |               |             |              |
| [[BERT]]: Pre-training of Deep Bidirectional Transformers for Language Undurstanding | 2019     | ACL           |               |             |              |
|                                                                                      |          |               |               |             |              |
|                                                                                      |          |               |               |             |              |

# Knowledge Graphs

| **Title**                                                                                             | **Conf'yy** | **Read Date** | **Summary**                                                                              | **Keywords** |
| ----------------------------------------------------------------------------------------------------- | ----------- | ------------- | ---------------------------------------------------------------------------------------- | ------------ |
| [[Plan-and-Solve Prompting]]: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models | ACL'23      | 250217        | 즉석적인 CoT 방식을 보안함. LLM이 추론하기 전, 미리 계획을 세워 이를 기반으로 문제 해결                                   |              |
| [[Reasoning on Graphs]]: Faithful and Interpretable Large Language Model Reasoning<br>                | ICLR'24     |               |                                                                                          |              |
| [[Graph-Constrained Reasoning]]: Faithful Reasoning on Knowledge Graphs with Large Language Models    | ArXiv'24    | 250226        | RoG의 후속작. KG-Trie와 KG specialized LLM을 사용하여 reasoning path 도출, 이를 기반으로 General LLM로 답 도출 |              |
|                                                                                                       |             |               |                                                                                          |              |
|                                                                                                       |             |               |                                                                                          |              |


그래프 엣지 제거하고 싶으면 이런식으로도 가능~
[보이고 싶은 링크](보이고 싶은 링크)
[숨기고 싶은 링크](숨기고 싶은 링크)

ACC, 시간