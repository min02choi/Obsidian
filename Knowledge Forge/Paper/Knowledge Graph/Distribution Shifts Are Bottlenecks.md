---
aliases:
---
논문 제목: Distribution Shifts Are Bottlenecks: Extensive Evaluation for Grounding Language Models to Knowledge Bases
![[Pasted image 20250314104912.png]]

***
### **1. 연구 배경 (Background)**

언어 모델(LMs)을 지식 베이스(KB)와 연결하는 것은 더 풍부하고 정확한 정보를 얻는 데 도움이 되지만, **데이터 분포의 변화(Distribution Shifts)** 로 인해 어려움을 겪음. 기존 벤치마크는 모델의 강건성을 공정하게 평가하지 못하고, 새로운 스키마(구조), 언어적 변형, 소수 샘플 학습(few-shot learning) 등에 대한 적응력이 부족함.

---

### **2. 연구 동기 및 아이디어 (Motivation & Idea)**

- **기존 연구의 한계점**:
    - 벤치마크는 대규모 지식 베이스(KB)의 복잡한 구조(schema)와 제한된 관찰 가능성을 반영하지 못함.
    - 모델 성능 평가가 단순한 F1/Hits@1 점수에 집중되어 실제 환경의 다양성을 반영하지 못함.
    - 분포 변화(Distribution Shift)에 따른 모델의 적응 능력을 고려하지 않음.

- **연구 동기**:
    - 기존 KBQA(지식 기반 질의 응답) 벤치마크의 한계를 극복하고 모델이 실제 환경에서도 강건하게 작동할 수 있도록 **평가 프로토콜을 개선**.
    - **데이터 증강 및 평가 기법**을 통해 LMs가 보다 넓은 도메인에서 잘 작동하도록 강화.

- **핵심 아이디어**:
    - KBQA 모델의 **강건성을 다각적으로 평가**(환경적, 언어적, 통합적, 모달 관점).
    - **새로운 평가 프로토콜을 개발**하여 모델의 실제 성능을 보다 정밀하게 분석.
    - **데이터 증강 기법(GAIN)** 을 활용해 모델의 일반화 성능 향상.

---

### **3. 주요 기여점 (Contribution)**

1. **강건성 평가 프레임워크 제안**:
    - 환경적(새로운 스키마), 언어적(다양한 표현), 통합적(데이터셋 간 전이), 모달(구조적 KB 적응) 네 가지 측면에서 LMs의 한계를 분석.

2. **새로운 데이터 증강 기법(GAIN) 개발**:
    - **Graph Search & Question Generation(GAIN)** 기법을 활용해 KB에서 새로운 질문을 생성하고, 기존 모델을 개선.

3. **대규모 실험 수행 및 모델 평가**:
    - 다양한 벤치마크(GrailQA, WebQSP, GraphQuestions, SimpleQuestions-Balance)에서 분포 변화에 따른 모델 성능을 평가.

4. **LLM(대형 언어 모델)의 한계 분석**:
    - LLM(GPT-3.5)의 In-Context Learning이 지식 베이스(KB) 환경에서 효과적이지 않음을 실험적으로 입증.

> **In-Context Learning vs Fine-Tunung**
> * **In-Context Learning**: 입력 프롬프트에 몇 가지 예제(few-shot examples)나 추가적인 정보(context)를 제공함으로써 모델이 마치 새로운 패턴을 학습한 것처럼 동작/대답을 유도하는 방법. *(모델의 가중치는 바뀌지 않음)*
> * **Fine-Tunung**: 모델을 새로운 데이터에 대해 직접 미세 조정하는 방식. *(모델의 가중치가 업데이트 됨)*

---

### **4. 방법론 (Method)**

- **1) 평가 프로토콜 설계**:
    - 기존 벤치마크의 한계를 극복하기 위해 환경적, 언어적, 통합적, 모달 측면에서 모델을 평가하는 새로운 방법론 제안.

- **2) 데이터 증강(GAIN) 방법 개발**:
    - KB에서 논리적 표현(Logic Form)과 자연어 질문을 생성하는 방법론.
    - 데이터 생성 단계를 4단계로 구성:
        1. **Graph Search**: KB에서 논리적 표현(Logic Form) 샘플링
        2. **Training Question Generator**: 기존 KBQA 데이터를 학습하여 질문 생성기 개발
        3. **Verbalization**: 생성된 논리적 표현을 자연어 질문으로 변환
        4. **Training Data Expansion**: 모델 학습을 위한 데이터 증강 수행

- **3) LLM(대형 언어 모델) 보조 학습 기법 도입**:
    - **Retrieval-Augmented Learning**: KB 문맥을 검색하여 LLM에 제공하여 학습 향상.

---

### **5. 주요 실험 결과 (Results)**

- **1) 환경적 측면**:
    - 분포 변화가 큰 경우(새로운 스키마) 모델 성능이 급격히 저하됨.
    - 제안된 GAIN 기법을 적용하면 일반화 성능이 향상됨(F1 점수 +2.5%).

- **2) 언어적 측면**:
    - 다양한 표현(Paraphrasing)에 대한 적응력 부족.
    - GAIN 기법을 적용하면 일부 개선되지만, 표현 다양성이 높은 데이터셋에서는 한계가 있음.

- **3) 통합적 측면**:
    - 훈련 데이터와 테스트 데이터 간 차이가 크면 성능 저하(WebQSP 데이터셋에서 F1: 29.8%).
    - 데이터 수집 방식의 차이가 모델 성능에 큰 영향을 미침.

- **4) 모달 측면**:
    - GPT-3.5 등의 LLM은 KB 환경에서 제대로 학습되지 않음.
    - 주어진 문맥(prompt)의 내용을 직접 복사하는 경향이 있으며, 창의적인 추론이 부족함.

---

### **6. 논문의 한계 및 향후 연구 방향 (Limitation & Insights)**

1. **데이터 증강의 한계**:
    - GAIN 기법이 일부 성능을 향상시키지만, 자연스러운 문장을 생성하는 데 한계가 있음.
    - 인간이 직접 제작한 데이터에 비해 표현의 다양성이 부족.

2. **다국어 KBQA 연구 부족**:
    - 대부분의 KBQA 연구는 영어에 집중되어 있음.
    - 다국어 환경에서의 모델 성능 평가 및 데이터 증강 기법 개발이 필요.

3. **대형 언어 모델(LLM) 기반 KBQA의 한계**:
    - LLM이 KB 환경에서 학습할 때, 단순히 제공된 문장을 복사하는 경향이 있음.
    - KB와 자연어의 격차를 줄이기 위해 Tool Learning, Multi-Step Planning 등의 방법이 필요.

---

### **결론 (Conclusion)**

- 기존 KBQA 연구는 모델 성능을 단순히 벤치마크 점수로 평가하지만, 실제 환경에서는 **분포 변화(Distribution Shifts)** 로 인해 성능이 저하됨.
- 본 연구는 **강건성 평가 프레임워크**를 제안하고, **데이터 증강(GAIN) 기법**을 통해 모델의 일반화 성능을 향상시킴.
- LLM(GPT-3.5)의 KB 환경에서의 한계를 분석하고, 미래 연구에서는 **더 강력한 데이터 수집 및 학습 방법**이 필요함을 강조.