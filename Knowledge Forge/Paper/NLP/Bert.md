# Abstract
BERT: Bidirectional Encoder Representations from Transformers
* Transformer 아키텍쳐를 기반으로 한 pre-trained model
* 문맥을 양방향으로 고려하여 학습을 함
* BERT모델은 하나의 output layer만 추가하여 Fine-Tunning 이 가능하다

pre-trained language를 downstream task에 적용하는 기존 두 가지 방법
* feature-based
	* 
* fine-tuning

***
# 1. Introduction


***
# 2. Related Work

## 2.1 Unsupervised Feature-based Approaches

## 2.2 Unsupervised Fine-tuning Approaches