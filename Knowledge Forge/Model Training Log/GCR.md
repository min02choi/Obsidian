---
aliases:
  - GCR Training Log
---

*ë…¼ë¬¸ ì œëª©: Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models
***

**ê²°ê³¼ íŒŒì¼ ê²½ë¡œ**
*/home/bys3158/model_test/GCR/graph-constrained-reasoning/resources*
![[Pasted image 20250227171316.png]]

**í›ˆë ¨ ë¡œê·¸ ê²½ë¡œ**
*/home/bys3158/model_test/GCR/graph-constrained-reasoning/z_history_log*
![[Pasted image 20250227171450.png]]
***
### ëª©ì 
* ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì‚¬ìš©í•˜ì—¬ ì§€ì‹ ê·¸ë˜í”„(KG) ìƒì—ì„œ ì‹ ë¢°ì„± ìˆëŠ” ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¸.
* ì§€ì‹ ê·¸ë˜í”„ì˜ êµ¬ì¡°ì  íŠ¹ì„±ì„ ëª¨ë¸ì— ë°˜ì˜í•˜ì—¬, ë³´ë‹¤ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶”ë¡ ì„ ì´ëŒì–´ë‚´ëŠ” ë°©ë²•ì„ ì œì‹œ

### ì‹¤í—˜ ê³¼ì •
 
KGì—ì„œ reasoning pathë¥¼ ë„ì¶œí•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ì¶”ë¡ ì„ í•˜ê³ ì í•¨

ì´ *ë‘ ë‹¨ê³„*ë¡œ ì´ë£¨ì–´ì§:
1. **Step 1: Graph-constrained decoding**
	* KG-specialized LLMì„ í™œìš©í•˜ì—¬ KGì—ì„œ KG-grounded reasoning paths and hypotheses answersë¥¼ ë„ì¶œí•˜ëŠ” ê³¼ì •
		* Used Model: Llama-3.1-8B
		* beam-search

2. **Step 2: Graph Inductive reasoning**
	* General LLMì„ í†µí•´ Step1ì—ì„œ ë„ì¶œí•œ resoning pathì— ëŒ€í•´ì„œ ìµœì¢… answerì„ ë„ì¶œí•˜ëŠ” ê³¼ì •
		* Used Model: Llama-3.1-8B. chatgpt-3.5-turbo


WebQSPì—ì„œ ì´ ì„¸ë²ˆì˜ ì„±ëŠ¥ ì‹¤í—˜ì„ í•¨(step1 + step2)
* Llama + ChatGPT (gpu4, rtx3090)
* Llama + Llama (gpu4, rtx4090)
* Llama + ChatGPT (gpu4, rtx4090)

CWQì—ì„œ ì´ ì„¸ë²ˆì˜ ì„±ëŠ¥ ì‹¤í—˜ì„ í•¨(step1 + step2)

***
## WebQSP ë°ì´í„°ì…‹
### 1. Llama + ChatGPT (rtx3090)
* ì¼ì: 25.02.19
#### ì‚¬ìš© GPU ë° ëª¨ë¸
* **ì„œë²„ ìœ„ì¹˜**: ë°ì´í„°ì„¼í„°
- **GPU ê°œìˆ˜**: 4
- **íŒŒí‹°ì…˜**: big_suma_rtx3090

- **Reasoning Path ê³¼ì •**: `Llama-3.1-8B`
- **Reasoning Finnal Answer**: `gpt-3.5-turbo`
- **ì‚¬ìš© ë°ì´í„°ì…‹**: rmanluo/RoG-webqspì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°(WebQTest-n)
    - **ì´ ë°ì´í„° ìˆ˜**: 1628ê°œ

#### STEP1: Graph-constrained Decoding
* Model: Llama-3.1-8B

**ì„±ëŠ¥ ì§€í‘œ:**
- **Accuracy: 78.50835**
- Hit: 92.19422
     ì¶”ê°€ ì„±ëŠ¥ì§€í‘œ í™•ì¸
        - F1: 58.08451783226196
        - Precision: 56.45388825474874
        - Recall: 77.88374665748543
        - Path F1: 54.956581743950935
        - Path Precision: 55.396849786831346
        - Path Recall: 72.49821259823082
        - Path Answer F1: 59.84054831682146
        - Path Answer Precision: 58.80381654813123
        - Path Answer Recall: 78.50798893908504

**í…ŒìŠ¤íŠ¸ ê²°ê³¼**
- **ì´ ë°ì´í„° ìˆ˜**: 1628ê°œ
- **ì „ì²´ ì†Œìš” ì‹œê°„**: 3h 17m 53s
- **ê°œë‹¹ í‰ê·  ì†Œìš” ì‹œê°„**: 7.29s
    - **ìµœì¥ ì†Œìš” ì‹œê°„**: `WebQTest-1395` (18.91s)
- **None ê²°ê³¼**: `WebQTest-521`
     WebQTest-521 Data
        ```json
        {
        	"id": "WebQTest-521",
        	"question": "who was anakin skywalker",
        	"answer": ["Ted Bracewell"],
        	"q_entity": ["g.125_cxx77"],
        	"a_entity": ["Ted Bracewell"],
        	"graph": [["Tatooine", "fictional_universe.fictional_setting.characters_that_have_lived_here", "g.125_cxx77"], ["m.011qx8t1", "film.performance.character", "g.125_cxx77"]],
        	"choices": []
        }
	        ```

#### STEP2: Graph Inductive reasoning
* Model: gpt-3.5-turbo

**ì„±ëŠ¥ì§€í‘œ:**
- Accuracy: 74.19227
- Hit: 89.68058
- F1: 70.34046
     ì¶”ê°€ ì„±ëŠ¥ì§€í‘œ í™•ì¸
        - Precision: 76.89861
        - Recall: 74.19227

**í…ŒìŠ¤íŠ¸ ê²°ê³¼**
- **ì´ ë°ì´í„° ìˆ˜**: 1628ê°œ
- **ì „ì²´ ì†Œìš” ì‹œê°„**: 2m 18s
- **ê°œë‹¹ í‰ê·  ì†Œìš” ì‹œê°„**: 0.085s

> STEP1 + STEP2 ìµœì¢… ë‹µì•ˆ ë„ì¶œ ì‹œê°„: 8s

***

### 2. Llama + Llama (rtx4090)
* ì¼ì: 25.02.20
#### ì‚¬ìš© GPU ë° ëª¨ë¸
* **ì„œë²„ ìœ„ì¹˜**: ë°ì´í„°ì„¼í„°
- **GPU ê°œìˆ˜**: 2
	- step1ì€ 2ê°œë¡œ ëŒë ¸ëŠ”ë° step2 í•˜ë ¤ë‹ˆê¹Œ 4090ì— ëŒ€ê¸°ìˆìŒ... gpu 2->1 í•´ë†“ìŒ..
- **íŒŒí‹°ì…˜**: big_suma_rtx4090

- **Reasoning Path ê³¼ì •**: `Llama-3.1-8B`
- **Reasoning Finnal Answer**: `Llama-3.1-8B`
- **ì‚¬ìš© ë°ì´í„°ì…‹**: rmanluo/RoG-webqspì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°(WebQTest-n)
    - **ì´ ë°ì´í„° ìˆ˜**: 1628ê°œ

#### STEP1: Graph-constrained Decoding
* Model: rmanluo/Llama-3.1-8B

**ì„±ëŠ¥ ì§€í‘œ:**
- Accuracy: 78.46735
- Hit: 92.19422
     ì¶”ê°€ ì„±ëŠ¥ì§€í‘œ í™•ì¸
        - F1: 58.087459371338234
        - Precision: 56.490131803592156
        - Recall: 77.84786951019872
        - Path F1: 54.93910371255629
        - Path Precision: 55.34536248426845
        - Path Recall: 72.51294634221367
        - Path Answer F1: 59.8008934544002
        - Path Answer Precision: 58.77801192183491
        - Path Answer Recall: 78.46698989054858

**í…ŒìŠ¤íŠ¸ ê²°ê³¼**
- **ì´ ë°ì´í„° ìˆ˜**: 1628ê°œ
- **ì „ì²´ ì†Œìš” ì‹œê°„**: 3h 17m 53s
- **ê°œë‹¹ í‰ê·  ì†Œìš” ì‹œê°„**: 7~8s
    - **ìµœì¥ ì†Œìš” ì‹œê°„**: `WebQTest-1395` (18.91s)
- **None ê²°ê³¼**: `WebQTest-521`
      WebQTest-521 Data
        ```json
        {
        	"id": "WebQTest-521",
        	"question": "who was anakin skywalker",
        	"answer": ["Ted Bracewell"],
        	"q_entity": ["g.125_cxx77"],
        	"a_entity": ["Ted Bracewell"],
        	"graph": [["Tatooine", "fictional_universe.fictional_setting.characters_that_have_lived_here", "g.125_cxx77"], ["m.011qx8t1", "film.performance.character", "g.125_cxx77"]],
        	"choices": []
        }
	        ```

#### STEP2: Graph Inductive reasoning
* Model: Llama-3.1-8B
> step1ì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì€ KG Based ëª¨ë¸ì´ê³ ... step2ëŠ” general llm(Llama, Gpt)ì„ã…œã…œã…œ ì•„ë‹ˆ ì´ê±¸ë¡œ ì‹œê°„ ê²ë‚˜ ëŒìŒ..

**ì„±ëŠ¥ì§€í‘œ:**
- Accuracy: 72.44364
- Hit: 87.65356
- F1: 69.95159
     ì¶”ê°€ ì„±ëŠ¥ì§€í‘œ í™•ì¸
        - Precision: 77.27516
        - Recall: 72.44364

**í…ŒìŠ¤íŠ¸ ê²°ê³¼**
- **ì´ ë°ì´í„° ìˆ˜**: 1628ê°œ
- **ì „ì²´ ì†Œìš” ì‹œê°„**: 12m 41s
- **ê°œë‹¹ í‰ê·  ì†Œìš” ì‹œê°„**: 0.47s

> STEP1 + STEP2 ìµœì¢… ë‹µì•ˆ ë„ì¶œ ì‹œê°„: ì•½ 8s

***
### 3. Llama + ChatGPT (rtx4090)
* ì¼ì: 25.02.20
#### ì‚¬ìš© GPU ë° ëª¨ë¸
* **ì„œë²„ ìœ„ì¹˜**: ë°ì´í„°ì„¼í„°
- **GPU ê°œìˆ˜**: 2
- **íŒŒí‹°ì…˜**: big_suma_rtx4090

- **Reasoning Path ê³¼ì •**: `Llama-3.1-8B`
- **Reasoning Finnal Answer**: `gpt-3.5-turbo`
- **ì‚¬ìš© ë°ì´í„°ì…‹**: rmanluo/RoG-webqspì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°(WebQTest-n)
    - **ì´ ë°ì´í„° ìˆ˜**: 1628ê°œ

#### STEP1: Graph-constrained Decoding
* Model: Llama-3.1-8B

Llama + Llama (rtx4090)ì—ì„œ ë§Œë“  ë™ì¼í•œ reasoning pathë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ìƒëµ

#### STEP2: Graph Inductive reasoning
* Model: gpt-3.5-turbo

**ì„±ëŠ¥ì§€í‘œ:**
- Accuracy: 73.78062
- Hit: 89.12776
- F1: 70.532074
     ì¶”ê°€ ì„±ëŠ¥ì§€í‘œ í™•ì¸
        - Precision: 77.23570
        - Recall: 73.78062

**í…ŒìŠ¤íŠ¸ ê²°ê³¼**
- **ì´ ë°ì´í„° ìˆ˜**: 1628ê°œ
- **ì „ì²´ ì†Œìš” ì‹œê°„**: 2m 16s
- **ê°œë‹¹ í‰ê·  ì†Œìš” ì‹œê°„**: 0.084s

> STEP1 + STEP2 ìµœì¢… ë‹µì•ˆ ë„ì¶œ ì‹œê°„: 

***

## CWQ ë°ì´í„°ì…‹

* ìš°ì„  ê¸°ë³¸ì ìœ¼ë¡œ ë°ì´í„°ì…‹ì´ ì»¤ì„œ 4090ìœ¼ë¡œëŠ” reasoning path ê³¼ì •ì´ ì§„í–‰ë˜ì§€ ì•ŠìŒ.
### Llama + ChatGPT (rtx3090)
ì¼ì: 25.02.27
#### ì‚¬ìš© GPU ë° ëª¨ë¸
* **ì„œë²„ ìœ„ì¹˜**: ë°ì´í„°ì„¼í„°
- **GPU ê°œìˆ˜**: 2
- **íŒŒí‹°ì…˜**: big_suma_rtx3090

- **Reasoning Path ê³¼ì •**: `Llama-3.1-8B`
- **Reasoning Finnal Answer**: `gpt-3.5-turbo`
- **ì‚¬ìš© ë°ì´í„°ì…‹**: rmanluo/RoG-cwqì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°(test)
    - **ì´ ë°ì´í„° ìˆ˜**: 3531ê°œ

#### STEP1: Graph-constrained Decoding
* Model: Llama-3.1-8B

**ì„±ëŠ¥ ì§€í‘œ:**
- **Accuracy: 63.39327**
- Hit: 67.6988
     ì¶”ê°€ ì„±ëŠ¥ì§€í‘œ í™•ì¸
        - F1: 39.89591525988563
        - Precision: 33.4333400974026
        - Recall: 63.044758275579305
        - Path F1: 35.820318694880925
        - Path Precision: 33.27493686868687
        - Path Recall: 50.06448307919947
        - Path Answer F1: 40.650900374836404
        - Path Answer Precision: 34.49338248556999
        - Path Answer Recall: 63.265436953519014

**í…ŒìŠ¤íŠ¸ ê²°ê³¼**
- **ì´ ë°ì´í„° ìˆ˜**: 3531ê°œ
- **ì „ì²´ ì†Œìš” ì‹œê°„**: 5h 55m 23s
- **ê°œë‹¹ í‰ê·  ì†Œìš” ì‹œê°„**: 6.04s
    - **ìµœì¥ ì†Œìš” ì‹œê°„**: `WebQTrn-261_c360906c6dbb01444144ff6ff216a168`, 66.9s
- **None ê²°ê³¼**: 11ê°œ (ì•”í˜¸í™” ë¬¸ì œ?)

#### STEP2: Graph Inductive reasoning
* Model: gpt-3.5-turbo

**ì„±ëŠ¥ì§€í‘œ:**
- Accuracy: 56.98023
- Hit: 62.33361
- F1: 51.92698
     ì¶”ê°€ ì„±ëŠ¥ì§€í‘œ í™•ì¸
        - Precision: 52.0043
        - Recall: 56.98023

**í…ŒìŠ¤íŠ¸ ê²°ê³¼**
- **ì´ ë°ì´í„° ìˆ˜**: 3531ê°œ
- **ì „ì²´ ì†Œìš” ì‹œê°„**: 4m 20s
- **ê°œë‹¹ í‰ê·  ì†Œìš” ì‹œê°„**: 0.0736s

> STEP1 + STEP2 ìµœì¢… ë‹µì•ˆ ë„ì¶œ ì‹œê°„: 6.04+0.073 = 6.113s
***
### Llama + Llama (rtx3090)
#### STEP1: Graph-constrained Decoding
* Model: Llama-3.1-8B

Llama + Llama (rtx4090)ì—ì„œ ë§Œë“  ë™ì¼í•œ reasoning pathë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ìƒëµ

#### STEP2: Graph Inductive reasoning
* Model: Llama-3.1-8B

**ì„±ëŠ¥ì§€í‘œ:**
- Accuracy: 55.064961
- Hit: 60.7759
- F1: 50.686945
     ì¶”ê°€ ì„±ëŠ¥ì§€í‘œ í™•ì¸
        - Precision: 51.0777
        - Recall: 55.06496

**í…ŒìŠ¤íŠ¸ ê²°ê³¼**
- **ì´ ë°ì´í„° ìˆ˜**: 3531ê°œ
- **ì „ì²´ ì†Œìš” ì‹œê°„**: 32m 15s
- **ê°œë‹¹ í‰ê·  ì†Œìš” ì‹œê°„**: 0.548s

> STEP1 + STEP2 ìµœì¢… ë‹µì•ˆ ë„ì¶œ ì‹œê°„: 6.04+0.548 = 6.588s

***
### ì¶”ê°€ ì‹¤í—˜: hop=3ìœ¼ë¡œ í–ˆì„ ë–„ì˜ ê²°ê³¼

#### STEP 1: Graph-constrained Decoding
**ì„±ëŠ¥ ì§€í‘œ:**
- Accuracy: 64.05749
- Hit: 68.46590
- F1: 41.387729

**í…ŒìŠ¤íŠ¸ ê²°ê³¼**
- **ì´ ë°ì´í„° ìˆ˜**: 3531ê°œ
- **ì „ì²´ ì†Œìš” ì‹œê°„**: 14h 59m 07s
- **ê°œë‹¹ í‰ê·  ì†Œìš” ì‹œê°„**: 15.28s

####  STEP2: Graph Inductive reasoning
    
**Llama 3.1**
	**ì„±ëŠ¥ ì§€í‘œ:**
	- Accuracy: 55.215945
	- Hit: 60.88926
	- F1: 50.98779
	
	**í…ŒìŠ¤íŠ¸ ê²°ê³¼**
	- **ì´ ë°ì´í„° ìˆ˜**: 3531ê°œ
	- **ì „ì²´ ì†Œìš” ì‹œê°„**: 29m 52s
	- **ê°œë‹¹ í‰ê·  ì†Œìš” ì‹œê°„**: 0.51s

**ChatGPT 3.5**
	**ì„±ëŠ¥ ì§€í‘œ:**
	- Accuracy: 57.63819
	- Hit: 62.843387
	- F1: 52.705949
	
	**í…ŒìŠ¤íŠ¸ ê²°ê³¼**
	- **ì´ ë°ì´í„° ìˆ˜**: 3531ê°œ
	- **ì „ì²´ ì†Œìš” ì‹œê°„**: 4m 26s
	- **ê°œë‹¹ í‰ê·  ì†Œìš” ì‹œê°„**: 0.075s

***

***
### GPT API cost

**GPT API í˜¸ì¶œ ì‹œ json í˜•íƒœ**
```json
{
    "role": "user",
    "content": "Where was George Washington Carver from?"
}
```
* ì—¬ê¸°ì„œ `content`ë¶€ë¶„ë§Œ input ê°’ìœ¼ë¡œ ë“¤ì–´ê°(input token ê°’ ê³„ì‚°)

**ChatGPT API ê¸°ë³¸ì ì¸ ê³¼ê¸ˆ ë‹¨ìœ„**
![[Pasted image 20250310132316.png]]
* 1M Token, chat-3.5-turboê¸°ì¤€ $0.5 (í•œí™” ì•½ 727ì›)
* ì˜ì–´ ë‹¨ì–´ 1ê°œëŠ” ì•½ 1~2ê°œì˜ tokenìœ¼ë¡œ ê³„ì‚°ë¨

**ğŸ“ŒWebQSP, CWQ ë°ì´í„°ì…‹ì— ëŒ€í•œ API ì‚¬ìš© ê²°ê³¼**
* ë‘ ë°ì´í„°ì…‹ ëª¨ë‘ batch ì‚¬ìš© ì•ˆí•˜ê³ , 1ì§ˆë¬¸ ë‹¹ 1ë²ˆì˜ api í˜¸ì¶œ
	* api í˜¸ì¶œ ë¹ˆë„ìˆ˜ì— ë”°ë¼ ê³¼ê¸ˆì´ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì—, ì§ˆë¬¸ì„ í•œë²ˆì— batchë¡œ ë¬¶ì–´ì„œ ì²˜ë¦¬í•˜ë©´ ê¸ˆì•¡ ì ˆì•½ ê°€ëŠ¥(ì•½ 50%)

**WebQSP(1628ê°œ)**
* request: 1.628k
* input tokens: 385.038k
* output tokens: 17.938k
* charges: 0.22$

```text
[WebQTest-0]
> query:  [{'role': 'user', 'content': 'Reasoning Paths:\n# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English\n# Answer:\nJamaican English\n# Reasoning Path:\nJamaica -> location.country.currency_used -> Jamaican dollar -> finance.currency.countries_used -> Jamaica\n# Answer:\nJamaica\n# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language\n# Answer:\nJamaican Creole English Language\n\nQuestion:\nwhat does jamaican people speak?\n\nBased on the reasoning paths, please answer the given question. Please keep the answer as simple as possible and only return answers. Please return each answer in a new line.'}]
> result:  Jamaican English
Jamaican Creole English Language
>>> tiktoken ì˜ˆìƒ í† í° ìˆ˜: 141
>>> ì‚¬ìš©ëœ í† í° ìˆ˜ - ì…ë ¥: 148, ì¶œë ¥: 13, ì´í•©: 161
```

**CWQ(3528ê°œ)**
* request: 3.528k
* input tokens: 910.226k
* output tokens: 26.346k
* charges: 0.49$

***
### Additional: Reasoning ë‹¨ê³„ì—ì„œ ë“¤ì–´ê°€ëŠ” reasoning path ê°œìˆ˜
* RoGì™€ì˜ ì‹œê°„ ì°¨ì´: ì—¬ê¸°ì„œ ë‚˜ì˜¤ëŠ”ê±´ê°€?

**WebQSP**
- Avg Path: 4.996
- Max Path: 10
- Min Path: 1
- path ë¹ˆë„ìˆ˜: 0, 86, 149, 274, 279, 210, 189, 146, 130, 78, 86 (ì™¼ìª½ë¶€í„° 0~10)

**CWQ**
- Avg Num of Path: 4.972
- Max Path Cnt: 10
- Min Path Cnt: 1
- path ë¹ˆë„ìˆ˜: 0, 103, 364, 475, 564, 633, 533, 416, 243, 118, 71 (ì™¼ìª½ë¶€í„° 0~10)

***
WebQTest-1 ë°ì´í„°(ì´ìƒí•¨!!)
* gpt-4o-mini

```text
> query:  [{'role': 'user', 'content': 'Reasoning Paths:\n# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nUnited States Representative\n# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative\n# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kc -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative\n# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States Representative\n# Reasoning Path:\nJames K. Polk -> common.topic.notable_types -> US President -> freebase.type_profile.equivalent_topic -> President of the United States\n# Answer:\nPresident of the United States\n\nQuestion:\nwhat did james k polk do before he was president?\n\nBased on the reasoning paths, please answer the given question. Please keep the answer as simple as possible and only return answers. Please return each answer in a new line.'}]
```


```text
result:  United States Representative  
United States Representative  
United States Representative  
United States Representative
```

```text
result:  United States Representative  
United States Representative  
United States Representative  
President of the United States  
United States Representative
>>> ì‚¬ìš©ëœ í† í° ìˆ˜ - ì…ë ¥: 292, ì¶œë ¥: 22, ì´í•©: 314
```

ê²°ê³¼ê°€ ì™œ ì´ëª¨ì–‘?


ì•”íŠ¼... 4oë¡œ ë‹¤ì‹œ reason path
í™˜ê²½
current
* request: 
* input tokens: 
* output tokens: 
* charges: 0.006$



í™˜ê²½
* **ì„œë²„ ìœ„ì¹˜**: ë°ì´í„°ì„¼í„°
- **GPU ê°œìˆ˜**: 2
- **íŒŒí‹°ì…˜**: big_suma_rtx4090

**WebQSP**
gpt-4o-mini API request ê´€ë ¨
* request: 1.628k
* input tokens: 382.564k
* output tokens: 196.23k
* charges: 0.06$
ë°ì´í„°ì…‹ ì¸¡ì • ê´€ë ¨(ì„±ëŠ¥ì§€í‘œ, í…ŒìŠ¤íŠ¸ ê²°ê³¼)
* Accuracy: 74.09896
- Hit: 89.1277
- F1: 71.4938
- ì´ ë°ì´í„° ìˆ˜: 1628ê°œ
- ì „ì²´ ì†Œìš” ì‹œê°„: 2m 2s
- ê°œë‹¹ í‰ê·  ì†Œìš” ì‹œê°„: 0.075s

**CWQ**
gpt-4o-mini API request ê´€ë ¨
* request: 3.531k
* input tokens: 906.083k
* output tokens: 29.870k
* charges: 0.134$
ë°ì´í„°ì…‹ ì¸¡ì • ê´€ë ¨(ì„±ëŠ¥ì§€í‘œ, í…ŒìŠ¤íŠ¸ ê²°ê³¼)
* Accuracy: 59.7157
- Hit: 65.2223
- F1: 55.1792
- ì´ ë°ì´í„° ìˆ˜: 3531ê°œ
- ì „ì²´ ì†Œìš” ì‹œê°„: 4m 37s
- ê°œë‹¹ í‰ê·  ì†Œìš” ì‹œê°„: 0.0785s

***
## Appendix

.sh íŒŒì¼ for step1

```sh
#!/bin/bash
#SBATCH --job-name=my_GCR
#SBATCH --output=step1_result4090_GCR.txt
#SBATCH --gres=gpu:2

export PYTHONPATH=$(pwd):$PYTHONPATH

MODEL_PATH=rmanluo/GCR-Meta-Llama-3.1-8B-Instruct
MODEL_NAME=$(basename "$MODEL_PATH")

python workflow/predict_paths_and_answers.py \
Â  --data_path rmanluo \
Â  --d RoG-webqsp \
Â  --split test \
Â  --index_path_length 2 \
Â  --model_name ${MODEL_NAME} \
Â  --model_path ${MODEL_PATH} \
Â  --k 10 \
Â  --prompt_mode zero-shot \
Â  --generation_mode group-beam \
Â  --attn_implementation flash_attention_2
```

.sh íŒŒì¼ for step2
```sh
#!/bin/bash
#SBATCH --job-name=my_GCR
#SBATCH --output=step2_result_GCR.txt
#SBATCH --gres=gpu:4

export PYTHONPATH=$(pwd):$PYTHONPATH

# MODEL_PATH=rmanluo/GCR-Meta-Llama-3.1-8B-Instruct
MODEL_NAME="gpt-3.5-turbo"
REASONING_PATH="results/GenPaths/RoG-webqsp/GCR-Meta-Llama-3.1-8B-Instruct/test/zero-shot-group-beam-k10-index_len2/predictions.jsonl"

python workflow/predict_final_answer.py \
Â  --data_path rmanluo \
Â  --d RoG-webqsp \
Â  --split test \
Â  --model_name $MODEL_NAME \
Â  --reasoning_path $REASONING_PATH \
Â  --add_path True \
Â  -n 10
```
* ìœ„ëŠ” gptë¥¼ ì‚¬ìš©í•œ ê²½ìš°

terminal pronpt ì˜ˆì‹œ
```text
sbatch  --time=7:00:00 -q big_qos -p suma_rtx4090 ./scripts/run2.sh
```
* ê·¼ë° í™•ì‹¤íˆ 4090ì€ í• ë‹¹ë°›ëŠ”ê±° ìì²´ë¡œë„ ì˜¤ë˜ ê±¸ë¦¬ë“œë¼...

```text
python workflow/predict_final_answer.py --data_path rmanluo --d RoG-webqsp --split test[:9] --model_name rmanluo/GCR-Meta-Llama-3.1-8B-Instruct --model_path rmanluo/GCR-Meta-Llama-3.1-8B-Instruct --reasoning_path "results/GenPaths/RoG-webqsp/GCR-Meta-Llama-3.1-8B-Instruct/test/zero-shot-group-beam-k10-index_len2/predictions.jsonl" --add_path True  -n 10
```
ì´ê±°ëŠ” ê·œí™˜ì˜¤ë¹ ê°€ ì•Œë ¤ì¤€ ë°©ì‹.

```text
srun --gres=gpu:1 --time=1-00:00:00 --pty bash -i
```
í„°ë¯¸ë„ ê°€ì§€ê³  ë†€ì´ ì¢€ ì–´ë µë‹¤


CWQ ë°ì´í„°ì…‹, meta-llama/Llama-3.1-8B-Instruct  step2 ëŒë¦¬ê¸°
```sh
#!/bin/bash
#SBATCH --job-name=my_GCR_llama
#SBATCH --output=step2_result_GCR_llama.txt
#SBATCH --gres=gpu:2
  
export PYTHONPATH=$(pwd):$PYTHONPATH

### Model Selection(Llama/GPT)
MODEL_PATH=meta-llama/Llama-3.1-8B-Instruct
MODEL_NAME=Llama-3.1-8B-Instruct
# MODEL_NAME="meta-llama/Llama-3.1-8B-Instruct"
# MODEL_NAME=$(basename "$MODEL_PATH")
# MODEL_NAME="gpt-3.5-turbo"
REASONING_PATH="results/GenPaths/RoG-cwq/GCR-Meta-Llama-3.1-8B-Instruct/test/zero-shot-group-beam-k10-index_len2/predictions.jsonl"

python workflow/predict_final_answer.py \
Â  --data_path rmanluo \
Â  --d RoG-cwq \
Â  --split test \
Â  --model_name ${MODEL_NAME} \
Â  --model_path ${MODEL_PATH} \
Â  --reasoning_path ${REASONING_PATH} \
Â  --add_path True \
Â  -n 10
```