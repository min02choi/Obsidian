**개요**
Google의 BERT 모델에서 사용된 방식으로, BPE와 비슷하지만 언어 모델 학습 과정에서 확률적 정보를 활용함.  
서브워드를 생성할 때, 전체 단어 시퀀스에서 최대 우도(MLE, Maximum Likelihood Estimation)를 기준으로 서브워드를 분할  

**장점**
* BPE보다 더 정교하게 서브워드를 생성하므로, 단어 분할이 문맥에 적합할 가능성이 높음  

**작동 방식**
1. 전체 텍스트를 문자 단위로 분리  
2. 가능한 모든 서브워드를 생성한 뒤, 이 서브워드가 데이터에 나타날 확률을 계산  
3. 언어 모델의 우도를 높이는 서브워드를 선택하며 어휘를 확장  
4. 특정 빈도 이하로 나타나는 희귀 서브워드는 분할하지 않고 문자 단위로 남김 
	* 예) unbelievable --> un, ##believe, ##able  
	 * ##는 해당 서브워드가 이전 서브워드와 연결됨을 나타냄  