---
aliases:
  - Byte-Pair Encodings
  - BPE
---


**개요**
원래는 데이터 압축 알고리즘으로 사용되었지만, NLP에서 토큰화 방식으로 확장됨.
자주 등장하는 문자 또는 문자 시퀀스 쌍을 결합해 새로운 서브워드를 생성.   

**단점**
문자 쌍 빈도를 기반으로 서브워드를 생성하므로, 언어의 문법적 정보는 고려하지 않음

**작동 방식**
1. 모든 단어를 문자(character) 단위로 분리  
	* 예: machine → m a c h i n e
2. 가장 많이 나타나는 문자 쌍(예: c와 h)을 하나의 서브워드로 결합  
	* 예: m a c h i n e → m a ch i n e  
3. 이를 반복해 자주 사용되는 문자 쌍을 점점 더 긴 서브워드로 만듦  
	* 예: m a ch i n e → ma ch i ne → mach ine  
4. 원하는 어휘 크기가 될 때까지 반복  